---
title: "LasR - Regression"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

Load required packages
 
```{r, message=F}
library(moleculaR)
library(rxn.cond.class)
library(ggplot2)
library(ggrepel)
library(parallel)
```

```{r, include=F}

model.subset.parallel <- function(data, out.col = dim(data)[2],
                                  min = 2, max = floor(dim(data)[1] / 5),
                                  results_name = 'results_df',
                                  folds = nrow(data), iterations = 1,
                                  cutoff = 0.65, cor.threshold = 1,
                                  min_models = 50, verbose = TRUE) {
  
  # Get the output variable and feature names
  output <- stringr::str_c("`", names(data[out.col]), "`")
  vars <- names(data[, -out.col])
  for (i in 1:length(vars)) {
    vars[i] <- stringr::str_c("`", vars[i], "`")
  }
  
  # Compute the correlation matrix for features
  cor_matrix <- cor(data[, -out.col])
  
  # Extract directory path from results_name
  results_dir <- dirname(results_name)
  if(results_dir == ".") results_dir <- getwd()
  
  # Clean up any existing result files with the same name pattern in this directory
  file_pattern <- paste0('^', basename(results_name), ".*\\.csv$")
  old_files <- list.files(path = results_dir, pattern = file_pattern, full.names = TRUE)
  if(length(old_files) > 0) {
    if(verbose) cat("Cleaning up", length(old_files), "old result files...\n")
    for(file in old_files) {
      unlink(file)
    }
  }
  
  # Track total models evaluated
  total_models_evaluated <- 0
  models_saved <- 0
  
  # Process each feature size in range
  for (i in min:max) {
    if(verbose) cat("Processing feature size:", i, "\n")
    
    # Safety check for combinations
    if(choose(length(vars), i) <= 0 || i > length(vars)) {
      if(verbose) cat("  Skipping - invalid feature size\n")
      next
    }
    
    # Calculate combinations in chunks to save memory
    chunk_size <- min(10000, choose(length(vars), i))
    total_combs <- choose(length(vars), i)
    
    if(verbose) {
      cat("  Total combinations:", total_combs, "\n")
      cat("  Using chunk size:", chunk_size, "\n")
    }
    
    # Track progress for this feature size
    models_saved_for_size <- 0
    
    # Process combinations in chunks
    for(chunk_start in seq(1, total_combs, by = chunk_size)) {
      chunk_end <- min(chunk_start + chunk_size - 1, total_combs)
      
      # Generate combinations for this chunk
      current_chunk <- utils::combn(vars, i, simplify = FALSE)[chunk_start:chunk_end]
      
      # Track formulas that pass correlation filter
      valid_formulas <- character()
      
      # Apply correlation filtering to each combination
      for(vars_combo in current_chunk) {
        if(length(vars_combo) > 1) {
          # Check correlation between variables
          clean_vars <- gsub("`", "", vars_combo)
          var_indices <- match(clean_vars, colnames(cor_matrix))
          var_cor <- cor_matrix[var_indices, var_indices]
          
          # Skip if any pair has correlation above threshold
          if(any(abs(var_cor[upper.tri(var_cor)]) > cor.threshold)) {
            next
          }
        }
        
        # Add to valid formulas
        valid_formulas <- c(valid_formulas, paste(output, "~", paste(vars_combo, collapse = " + ")))
      }
      
      if(length(valid_formulas) == 0) {
        next
      }
      
      # Calculate R² in parallel for valid formulas
      r_squared_results <- parallel::mclapply(
        valid_formulas,
        function(formula) {
          result <- try(summary(lm(formula, data = data))$r.squared, silent = TRUE)
          if(inherits(result, "try-error")) {
            return(NA)
          }
          return(result)
        },
        mc.cores = max(1, parallel::detectCores() - 1)
      )
      
      # Filter out failed models and create results table
      valid_indices <- !sapply(r_squared_results, is.na)
      valid_formulas <- valid_formulas[valid_indices]
      valid_r2 <- unlist(r_squared_results[valid_indices])
      
      if(length(valid_r2) == 0) {
        next
      }
      
      total_models_evaluated <- total_models_evaluated + length(valid_r2)
      
      # Create results dataframe
      results_df <- data.frame(
        formula = valid_formulas,
        R.sq = valid_r2,
        stringsAsFactors = FALSE
      )
      
      # Save all model results to temporary file
      timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S_%OS3")
      file_name <- file.path(results_dir, paste0(basename(results_name), "_", i, "_", timestamp, ".csv"))
      write.csv(results_df, file = file_name, row.names = FALSE)
      models_saved_for_size <- models_saved_for_size + nrow(results_df)
      models_saved <- models_saved + nrow(results_df)
    }
    
    if(verbose) {
      cat("  Saved", models_saved_for_size, "models for feature size", i, "\n")
    }
  }
  
  if(verbose) {
    cat("Total models evaluated:", total_models_evaluated, "\n")
    cat("Models saved to temporary files:", models_saved, "\n")
  }
  
  # Get list of all result files from the correct directory
  file_list <- list.files(
    path = results_dir, 
    pattern = paste0('^', basename(results_name), ".*\\.csv$"), 
    full.names = TRUE
  )
  
  if(length(file_list) == 0) {
    warning("No model result files found in directory:", results_dir)
    return(data.frame(
      formula = character(),
      R.sq = numeric(),
      Q.sq = numeric(),
      MAE = numeric(),
      Model = integer(),
      stringsAsFactors = FALSE
    ))
  }
  
  if(verbose) {
    cat("Found", length(file_list), "temporary files with model results\n")
    cat("Finding models with R² ≥", cutoff, "\n")
  }
  
  # First attempt: read files and collect models above initial cutoff
  all_models <- data.frame(formula = character(), R.sq = numeric(), stringsAsFactors = FALSE)
  r_squared_values <- numeric()
  
  for(file in file_list) {
    tryCatch({
      # Read the file
      temp_df <- utils::read.csv(file, stringsAsFactors = FALSE)
      
      # Store all R² values for potential cutoff adjustment
      if(ncol(temp_df) >= 2) {
        r_squared_values <- c(r_squared_values, temp_df[[2]])
        
        # Filter models above cutoff
        models_above_cutoff <- temp_df[temp_df[[2]] >= cutoff, , drop = FALSE]
        if(nrow(models_above_cutoff) > 0) {
          all_models <- rbind(all_models, models_above_cutoff)
        }
      }
    }, error = function(e) {
      warning("Error reading file: ", file, " - ", e$message)
    })
  }
  
  # If not enough models, adaptively adjust cutoff
  if(nrow(all_models) < min_models && length(r_squared_values) > 0) {
    adapted_cutoff <- cutoff
    
    while(nrow(all_models) < min_models && adapted_cutoff > 0.1) {
      # Lower the cutoff
      adapted_cutoff <- adapted_cutoff - 0.05
      
      if(verbose) {
        cat("Adjusting cutoff to", adapted_cutoff, "to find more models\n")
      }
      
      # Clear previous models
      all_models <- data.frame(formula = character(), R.sq = numeric(), stringsAsFactors = FALSE)
      
      # Re-read files with new cutoff
      for(file in file_list) {
        tryCatch({
          temp_df <- utils::read.csv(file, stringsAsFactors = FALSE)
          if(ncol(temp_df) >= 2) {
            models_above_cutoff <- temp_df[temp_df[[2]] >= adapted_cutoff, , drop = FALSE]
            if(nrow(models_above_cutoff) > 0) {
              all_models <- rbind(all_models, models_above_cutoff)
            }
          }
        }, error = function(e) {
          warning("Error reading file: ", file, " - ", e$message)
        })
      }
    }
    
    # If still not enough models, just take the top ones
    if(nrow(all_models) < min_models && length(r_squared_values) >= min_models) {
      if(verbose) {
        cat("Taking top", min_models, "models by R²\n")
      }
      
      cutoff_value <- sort(r_squared_values, decreasing = TRUE)[min(min_models, length(r_squared_values))]
      
      # Clear previous models
      all_models <- data.frame(formula = character(), R.sq = numeric(), stringsAsFactors = FALSE)
      
      # Re-read files with cutoff based on top models
      for(file in file_list) {
        tryCatch({
          temp_df <- utils::read.csv(file, stringsAsFactors = FALSE)
          if(ncol(temp_df) >= 2) {
            models_above_cutoff <- temp_df[temp_df[[2]] >= cutoff_value, , drop = FALSE]
            if(nrow(models_above_cutoff) > 0) {
              all_models <- rbind(all_models, models_above_cutoff)
            }
          }
        }, error = function(e) {
          warning("Error reading file: ", file, " - ", e$message)
        })
      }
    }
  }
  
  # If still no models, take any available
  if(nrow(all_models) == 0 && length(r_squared_values) > 0) {
    if(verbose) {
      cat("No models above cutoff. Taking all available models.\n")
    }
    
    # Read all models without filtering
    for(file in file_list) {
      tryCatch({
        temp_df <- utils::read.csv(file, stringsAsFactors = FALSE)
        if(ncol(temp_df) >= 2) {
          all_models <- rbind(all_models, temp_df)
        }
      }, error = function(e) {
        warning("Error reading file: ", file, " - ", e$message)
      })
    }
  }
  
  # If still no models, handle the error
  if(nrow(all_models) == 0) {
    warning("No valid models found despite adaptive cutoff.")
    
    # Clean up temporary files
    if(verbose) cat("Cleaning up temporary files...\n")
    for(file in file_list) unlink(file)
    
    return(data.frame(
      formula = character(),
      R.sq = numeric(),
      Q.sq = numeric(),
      MAE = numeric(),
      Model = integer(),
      stringsAsFactors = FALSE
    ))
  }
  
  # Sort models by R²
  all_models <- all_models[order(all_models[[2]], decreasing = TRUE), , drop = FALSE]
  names(all_models) <- c("formula", "R.sq")
  
  # Keep only top models for cross-validation
  if(nrow(all_models) > min_models) {
    all_models <- all_models[1:min_models, , drop = FALSE]
  }
  
  if(verbose) {
    cat("Running cross-validation on", nrow(all_models), "models...\n")
  }
  
  # Perform cross-validation on selected models
  q2.list <- list()
  mae.list <- list()
  
  for(i in 1:nrow(all_models)) {
    if(verbose && i %% 10 == 0) {
      cat("  Cross-validating model", i, "of", nrow(all_models), "\n")
    }
    
    stts <- model.cv(
      formula = all_models$formula[i],
      data = data,
      out.col = out.col,
      folds = folds,
      iterations = iterations
    )
    q2.list[[i]] <- stts[[2]]
    mae.list[[i]] <- stts[[1]]
  }
  
  # Add Q-squared and MAE to the final results
  all_models$Q.sq <- unlist(q2.list)
  all_models$MAE <- unlist(mae.list)
  
  # Sort by Q-squared and add model numbers
  final_models <- all_models[order(all_models$Q.sq, decreasing = TRUE), , drop = FALSE]
  final_models$Model <- seq_len(nrow(final_models))
  
  # Clean up temporary files
  if(verbose) cat("Cleaning up temporary files...\n")
  for(file in file_list) unlink(file)
  
  # Return top 15 models or all models if less than 15
  if(nrow(final_models) > 15) {
    return(final_models[1:15, ])
  } else {
    return(final_models)
  }
}


models.list.parallel <- function(dataset,
                                 min = 2,
                                 max = NULL,
                                 leave.out = '',
                                 folds = nrow(read.csv(dataset)), 
                                 iterations = 1,
                                 cutoff = 0.50,
                                 cor.threshold = 1.0, # Allow any correlation by default
                                 min_models = 50,
                                 verbose = TRUE) {
  
  # Create directory name based on parameters and timestamp
  dir_name <- paste0(
    tools::file_path_sans_ext(basename(dataset)),
    '_min', as.character(min),
    '_max', ifelse(is.null(max), "auto", as.character(max)),
    '_', format(Sys.time(), "%Y%m%d_%H%M%S")
  )
  
  # Create directory if it doesn't exist
  if (!dir.exists(dir_name)) {
    dir.create(dir_name)
  }
  
  # Get absolute path to ensure consistent file handling
  dir_path <- normalizePath(dir_name)
  
  # Modify results_name to include directory path
  results_file_path <- file.path(dir_path, "results_df")
  
  if(verbose) {
    cat("Reading and processing dataset:", dataset, "\n")
    cat("Results will be saved in directory:", dir_name, "\n")
  }
  
  # Read and preprocess data
  mod_data <- tryCatch({
    utils::read.csv(dataset, stringsAsFactors = FALSE, check.names = FALSE)
  }, error = function(e) {
    stop("Error reading dataset: ", e$message)
  })
  
  if(ncol(mod_data) <= 1) {
    stop("Dataset must have at least two columns (row names and one variable)")
  }
  
  # Store row names (first column) and remove it
  RN <- mod_data[,1]
  mod_data <- mod_data[,-1]
  
  # Remove rows with missing values
  mod_data <- mod_data[complete.cases(mod_data), ]
  
  if(nrow(mod_data) == 0) {
    stop("No complete cases found in the dataset after removing missing values")
  }
  
  CN <- names(mod_data)
  
  # Scale predictors (all columns except the last one)
  predictors <- mod_data[, 1:(ncol(mod_data) - 1), drop = FALSE]
  outcome <- mod_data[, ncol(mod_data)]
  
  # Scale remaining predictors
  if(ncol(predictors) > 0) {
    scaled_predictors <- scale(predictors, center = TRUE, scale = TRUE)
    
    # Check for NaN or Inf values after scaling
    if(any(is.na(scaled_predictors)) || any(is.infinite(scaled_predictors))) {
      warning("Scaling produced NaN or Inf values. Using original unscaled predictors.")
      scaled_predictors <- as.data.frame(predictors)
    } else {
      scaled_predictors <- as.data.frame(scaled_predictors)
    }
    
    # Combine scaled predictors with outcome
    mod_data <- data.frame(scaled_predictors, outcome = outcome)
    names(mod_data) <- c(names(scaled_predictors), CN[length(CN)])
  } else {
    warning("No valid predictor columns remain.")
    return(data.frame())
  }
  
  row.names(mod_data) <- RN
  
  # Extract observations to leave out
  if(length(leave.out) > 0 && any(leave.out != '')) {
    leave_out_indices <- row.names(mod_data) %in% leave.out
    if(any(leave_out_indices)) {
      pred.data <- mod_data[leave_out_indices, , drop = FALSE]
      mod_data <- mod_data[!leave_out_indices, , drop = FALSE]
      
      if(verbose) {
        cat("Left out", nrow(pred.data), "samples for testing\n")
      }
    } else {
      warning("Specified leave.out samples not found in dataset")
      pred.data <- NULL
    }
  } else {
    pred.data <- NULL
  }
  
  # Set default max if not provided
  if (is.null(max)) {
    max <- min(floor(nrow(mod_data) / 5), 5)  # Cap at 5 for small datasets
    if(verbose) cat("Setting max number of features to:", max, "\n")
  }
  
  # Default number of folds - cap at number of observations
  folds <- min(folds, nrow(mod_data))
  if(verbose) cat("Setting number of cross-validation folds to:", folds, "\n")
  
  if(verbose) {
    cat("Running model subset selection with parameters:\n")
    cat("- Min features:", min, "\n")
    cat("- Max features:", max, "\n")
    cat("- Initial R² cutoff:", cutoff, "\n")
    cat("- Correlation threshold:", cor.threshold, "\n")
    cat("- Min models for CV:", min_models, "\n")
    cat("- CV folds:", folds, "\n")
    cat("- CV iterations:", iterations, "\n")
    cat("- Number of observations:", nrow(mod_data), "\n")
    cat("- Number of variables:", ncol(mod_data) - 1, "\n")
  }
  
  # Check if there are enough observations and variables
  if(nrow(mod_data) < folds) {
    warning("Number of observations (", nrow(mod_data), ") is less than number of folds (", folds, ").",
            "Setting folds to ", nrow(mod_data))
    folds <- nrow(mod_data)
  }
  
  if(ncol(mod_data) - 1 < min) {
    stop("Too few variables (", ncol(mod_data) - 1, ") to meet minimum feature requirement (", min, ")")
  }
  
  # Run model.subset.parallel with robust error handling
  models <- tryCatch({
    model.subset.parallel(
      data = mod_data,
      out.col = ncol(mod_data),  # Last column
      min = min,
      max = max,
      results_name = results_file_path,
      folds = folds, 
      iterations = iterations,
      cutoff = cutoff,
      cor.threshold = cor.threshold,
      min_models = min_models,
      verbose = verbose
    )
  }, error = function(e) {
    cat("Error in model.subset.parallel:", e$message, "\n")
    
    # Return empty data frame with appropriate structure
    data.frame(
      formula = character(),
      R.sq = numeric(),
      Q.sq = numeric(),
      MAE = numeric(),
      Model = integer(),
      stringsAsFactors = FALSE
    )
  })
  
  if(nrow(models) == 0) {
    warning("No models found. Check your data and parameters.")
  } else {
    if(verbose) {
      cat("\nFound", nrow(models), "models.\n")
      cat("Top", min(5, nrow(models)), "models by Q²:\n")
      print(knitr::kable(head(models, 5)))
    }
    
    # Save models list in the results directory
    output_file <- file.path(
      dir_path,
      paste0(
        tools::file_path_sans_ext(basename(dataset)),
        '_models_list.csv'
      )
    )
    
    utils::write.csv(models, output_file, row.names = FALSE)
    
    if(verbose) {
      cat("\nResults saved to:", output_file, "\n")
    }
    
    # If there are left-out samples and at least one model, make predictions
    if(!is.null(pred.data) && nrow(pred.data) > 0 && nrow(models) > 0) {
      best_model <- tryCatch({
        lm(as.formula(models$formula[1]), data = mod_data)
      }, error = function(e) {
        warning("Error fitting best model for predictions: ", e$message)
        NULL
      })
      
      if(!is.null(best_model)) {
        predictions <- tryCatch({
          predict(best_model, newdata = pred.data)
        }, error = function(e) {
          warning("Error making predictions: ", e$message)
          NULL
        })
        
        if(!is.null(predictions)) {
          actual_values <- pred.data[[ncol(pred.data)]]
          
          prediction_results <- data.frame(
            Sample = row.names(pred.data),
            Actual = actual_values,
            Predicted = predictions,
            Error = actual_values - predictions
          )
          
          # Save predictions
          pred_file <- file.path(
            dir_path,
            paste0(
              tools::file_path_sans_ext(basename(dataset)),
              '_predictions.csv'
            )
          )
          
          utils::write.csv(prediction_results, pred_file, row.names = FALSE)
          
          if(verbose) {
            cat("\nPredictions for left-out samples saved to:", pred_file, "\n")
          }
        }
      }
    }
  }
  
  return(models)
}


# Windows-compatible version of model.subset.parallel
model.subset.parallel.windows <- function(data, out.col = dim(data)[2],
                                          min = 2, max = floor(dim(data)[1] / 5),
                                          results_name = 'results_df',
                                          folds = nrow(data), iterations = 1,
                                          cutoff = 0.65, cor.threshold = 0.7,
                                          min_models = 50, verbose = TRUE) {
  
  # Get the output variable and feature names
  output <- stringr::str_c("`", names(data[out.col]), "`")
  vars <- names(data[, -out.col])
  for (i in 1:length(vars)) {
    vars[i] <- stringr::str_c("`", vars[i], "`")
  }
  
  # Compute the correlation matrix for features
  cor_matrix <- cor(data[, -out.col])
  
  # Extract directory path from results_name
  results_dir <- dirname(results_name)
  if(results_dir == ".") results_dir <- getwd()
  
  # Clean up any existing result files with the same name pattern in this directory
  file_pattern <- paste0('^', basename(results_name), ".*\\.csv$")
  old_files <- list.files(path = results_dir, pattern = file_pattern, full.names = TRUE)
  if(length(old_files) > 0) {
    if(verbose) cat("Cleaning up", length(old_files), "old result files...\n")
    for(file in old_files) {
      unlink(file)
    }
  }
  
  # Track total models evaluated
  total_models_evaluated <- 0
  models_saved <- 0
  
  # Process each feature size in range
  for (i in min:max) {
    if(verbose) cat("Processing feature size:", i, "\n")
    
    # Safety check for combinations
    if(choose(length(vars), i) <= 0 || i > length(vars)) {
      if(verbose) cat("  Skipping - invalid feature size\n")
      next
    }
    
    # Calculate combinations in chunks to save memory
    chunk_size <- min(10000, choose(length(vars), i))
    total_combs <- choose(length(vars), i)
    
    if(verbose) {
      cat("  Total combinations:", total_combs, "\n")
      cat("  Using chunk size:", chunk_size, "\n")
    }
    
    # Track progress for this feature size
    models_saved_for_size <- 0
    
    # Process combinations in chunks
    for(chunk_start in seq(1, total_combs, by = chunk_size)) {
      chunk_end <- min(chunk_start + chunk_size - 1, total_combs)
      
      # Generate combinations for this chunk
      current_chunk <- utils::combn(vars, i, simplify = FALSE)[chunk_start:chunk_end]
      
      # Track formulas that pass correlation filter
      valid_formulas <- character()
      
      # Apply correlation filtering to each combination
      for(vars_combo in current_chunk) {
        if(length(vars_combo) > 1) {
          # Check correlation between variables
          clean_vars <- gsub("`", "", vars_combo)
          var_indices <- match(clean_vars, colnames(cor_matrix))
          var_cor <- cor_matrix[var_indices, var_indices]
          
          # Skip if any pair has correlation above threshold
          if(any(abs(var_cor[upper.tri(var_cor)]) > cor.threshold)) {
            next
          }
        }
        
        # Add to valid formulas
        valid_formulas <- c(valid_formulas, paste(output, "~", paste(vars_combo, collapse = " + ")))
      }
      
      if(length(valid_formulas) == 0) {
        next
      }
      
      # WINDOWS MODIFICATION: Use parLapply instead of mclapply
      # Create a cluster for parallel processing
      n_cores <- max(1, parallel::detectCores() - 1)
      cl <- parallel::makeCluster(n_cores)
      
      # Export necessary objects to the cluster
      parallel::clusterExport(cl, c("data"), envir = environment())
      
      # Calculate R² in parallel for valid formulas
      r_squared_results <- parallel::parLapply(
        cl,
        valid_formulas,
        function(formula) {
          result <- try(summary(lm(formula, data = data))$r.squared, silent = TRUE)
          if(inherits(result, "try-error")) {
            return(NA)
          }
          return(result)
        }
      )
      
      # Close the cluster
      parallel::stopCluster(cl)
      
      # Filter out failed models and create results table
      valid_indices <- !sapply(r_squared_results, is.na)
      valid_formulas <- valid_formulas[valid_indices]
      valid_r2 <- unlist(r_squared_results[valid_indices])
      
      if(length(valid_r2) == 0) {
        next
      }
      
      total_models_evaluated <- total_models_evaluated + length(valid_r2)
      
      # Create results dataframe
      results_df <- data.frame(
        formula = valid_formulas,
        R.sq = valid_r2,
        stringsAsFactors = FALSE
      )
      
      # Save all model results to temporary file
      timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S_%OS3")
      file_name <- file.path(results_dir, paste0(basename(results_name), "_", i, "_", timestamp, ".csv"))
      write.csv(results_df, file = file_name, row.names = FALSE)
      models_saved_for_size <- models_saved_for_size + nrow(results_df)
      models_saved <- models_saved + nrow(results_df)
    }
    
    if(verbose) {
      cat("  Saved", models_saved_for_size, "models for feature size", i, "\n")
    }
  }
  
  if(verbose) {
    cat("Total models evaluated:", total_models_evaluated, "\n")
    cat("Models saved to temporary files:", models_saved, "\n")
  }
  
  # Get list of all result files from the correct directory
  file_list <- list.files(
    path = results_dir, 
    pattern = paste0('^', basename(results_name), ".*\\.csv$"), 
    full.names = TRUE
  )
  
  if(length(file_list) == 0) {
    warning("No model result files found in directory:", results_dir)
    return(data.frame(
      formula = character(),
      R.sq = numeric(),
      Q.sq = numeric(),
      MAE = numeric(),
      Model = integer(),
      stringsAsFactors = FALSE
    ))
  }
  
  if(verbose) {
    cat("Found", length(file_list), "temporary files with model results\n")
    cat("Finding models with R² ≥", cutoff, "\n")
  }
  
  # First attempt: read files and collect models above initial cutoff
  all_models <- data.frame(formula = character(), R.sq = numeric(), stringsAsFactors = FALSE)
  r_squared_values <- numeric()
  
  for(file in file_list) {
    tryCatch({
      # Read the file
      temp_df <- utils::read.csv(file, stringsAsFactors = FALSE)
      
      # Store all R² values for potential cutoff adjustment
      if(ncol(temp_df) >= 2) {
        r_squared_values <- c(r_squared_values, temp_df[[2]])
        
        # Filter models above cutoff
        models_above_cutoff <- temp_df[temp_df[[2]] >= cutoff, , drop = FALSE]
        if(nrow(models_above_cutoff) > 0) {
          all_models <- rbind(all_models, models_above_cutoff)
        }
      }
    }, error = function(e) {
      warning("Error reading file: ", file, " - ", e$message)
    })
  }
  
  # If not enough models, adaptively adjust cutoff
  if(nrow(all_models) < min_models && length(r_squared_values) > 0) {
    adapted_cutoff <- cutoff
    
    while(nrow(all_models) < min_models && adapted_cutoff > 0.1) {
      # Lower the cutoff
      adapted_cutoff <- adapted_cutoff - 0.05
      
      if(verbose) {
        cat("Adjusting cutoff to", adapted_cutoff, "to find more models\n")
      }
      
      # Clear previous models
      all_models <- data.frame(formula = character(), R.sq = numeric(), stringsAsFactors = FALSE)
      
      # Re-read files with new cutoff
      for(file in file_list) {
        tryCatch({
          temp_df <- utils::read.csv(file, stringsAsFactors = FALSE)
          if(ncol(temp_df) >= 2) {
            models_above_cutoff <- temp_df[temp_df[[2]] >= adapted_cutoff, , drop = FALSE]
            if(nrow(models_above_cutoff) > 0) {
              all_models <- rbind(all_models, models_above_cutoff)
            }
          }
        }, error = function(e) {
          warning("Error reading file: ", file, " - ", e$message)
        })
      }
    }
    
    # If still not enough models, just take the top ones
    if(nrow(all_models) < min_models && length(r_squared_values) >= min_models) {
      if(verbose) {
        cat("Taking top", min_models, "models by R²\n")
      }
      
      cutoff_value <- sort(r_squared_values, decreasing = TRUE)[min(min_models, length(r_squared_values))]
      
      # Clear previous models
      all_models <- data.frame(formula = character(), R.sq = numeric(), stringsAsFactors = FALSE)
      
      # Re-read files with cutoff based on top models
      for(file in file_list) {
        tryCatch({
          temp_df <- utils::read.csv(file, stringsAsFactors = FALSE)
          if(ncol(temp_df) >= 2) {
            models_above_cutoff <- temp_df[temp_df[[2]] >= cutoff_value, , drop = FALSE]
            if(nrow(models_above_cutoff) > 0) {
              all_models <- rbind(all_models, models_above_cutoff)
            }
          }
        }, error = function(e) {
          warning("Error reading file: ", file, " - ", e$message)
        })
      }
    }
  }
  
  # If still no models, take any available
  if(nrow(all_models) == 0 && length(r_squared_values) > 0) {
    if(verbose) {
      cat("No models above cutoff. Taking all available models.\n")
    }
    
    # Read all models without filtering
    for(file in file_list) {
      tryCatch({
        temp_df <- utils::read.csv(file, stringsAsFactors = FALSE)
        if(ncol(temp_df) >= 2) {
          all_models <- rbind(all_models, temp_df)
        }
      }, error = function(e) {
        warning("Error reading file: ", file, " - ", e$message)
      })
    }
  }
  
  # If still no models, handle the error
  if(nrow(all_models) == 0) {
    warning("No valid models found despite adaptive cutoff.")
    
    # Clean up temporary files
    if(verbose) cat("Cleaning up temporary files...\n")
    for(file in file_list) unlink(file)
    
    return(data.frame(
      formula = character(),
      R.sq = numeric(),
      Q.sq = numeric(),
      MAE = numeric(),
      Model = integer(),
      stringsAsFactors = FALSE
    ))
  }
  
  # Sort models by R²
  all_models <- all_models[order(all_models[[2]], decreasing = TRUE), , drop = FALSE]
  names(all_models) <- c("formula", "R.sq")
  
  # Keep only top models for cross-validation
  if(nrow(all_models) > min_models) {
    all_models <- all_models[1:min_models, , drop = FALSE]
  }
  
  if(verbose) {
    cat("Running cross-validation on", nrow(all_models), "models...\n")
  }
  
  # Perform cross-validation on selected models
  q2.list <- list()
  mae.list <- list()
  
  for(i in 1:nrow(all_models)) {
    if(verbose && i %% 10 == 0) {
      cat("  Cross-validating model", i, "of", nrow(all_models), "\n")
    }
    
    # WINDOWS MODIFICATION: Use Windows-specific version of model.cv.parallel
    stts <- model.cv(
      formula = all_models$formula[i],
      data = data,
      out.col = out.col,
      folds = folds,
      iterations = iterations
    )
    q2.list[[i]] <- stts[[2]]
    mae.list[[i]] <- stts[[1]]
  }
  
  # Add Q-squared and MAE to the final results
  all_models$Q.sq <- unlist(q2.list)
  all_models$MAE <- unlist(mae.list)
  
  # Sort by Q-squared and add model numbers
  final_models <- all_models[order(all_models$Q.sq, decreasing = TRUE), , drop = FALSE]
  final_models$Model <- seq_len(nrow(final_models))
  
  # Clean up temporary files
  if(verbose) cat("Cleaning up temporary files...\n")
  for(file in file_list) unlink(file)
  
  # Return top 15 models or all models if less than 15
  if(nrow(final_models) > 15) {
    return(final_models[1:15, ])
  } else {
    return(final_models)
  }
}


# Function to allow seamless integration with any OS
if(.Platform$OS.type == "windows") {
  # Replace original functions with Windows versions
  assign("model.subset.parallel.original", model.subset.parallel, envir = .GlobalEnv)
  
  # Assign Windows versions
  assign("model.subset.parallel", model.subset.parallel.windows, envir = .GlobalEnv)
}

oos_validation_table <- function(oos_data, 
                                 plot.title = "Out-of-Sample Validation",
                                 subtitle = "Predicted vs Measured Values",
                                 conformation = "",
                                 error_threshold = 20) {  # Threshold to consider errors acceptable
  
  # Required packages
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("ggplot2 package is required")
  }
  
  # Make a copy of the input data
  df <- oos_data
  
  # Add a column to mark errors below threshold as acceptable
  df$Acceptable <- abs(df$`OOS Error`) <= error_threshold
  
  # Calculate the percentage of acceptable predictions
  accuracy_pct <- round(mean(df$Acceptable) * 100, 1)
  
  # Calculate mean absolute error (MAE)
  mae <- round(mean(abs(df$`OOS Error`)), 1)
  
  # Number of rows
  n_rows <- nrow(df)
  
  # Format the ggplot - starting with an empty plot
  plot <- ggplot2::ggplot() +
    
    # Set up the grid for the table - with equal-height cells
    # Bottom of table at y=0, top of table at y=n_rows
    
    # Vertical grid lines (columns)
    ggplot2::geom_segment(
      data = data.frame(x = c(0, 1, 2, 3, 4)),
      mapping = ggplot2::aes(
        x = x, xend = x,
        y = 0, yend = n_rows
      ),
      color = "black", size = 0.5
    ) +
    
    # Horizontal grid lines (rows) - with EQUAL height cells
    ggplot2::geom_segment(
      data = data.frame(y = 0:n_rows),
      mapping = ggplot2::aes(
        x = 0, xend = 4,
        y = y, yend = y
      ),
      color = "black", size = 0.5
    ) +
    
    # Column headers directly at the top of the table
    ggplot2::geom_text(
      data = data.frame(
        x = c(0.5, 1.5, 2.5, 3.5),
        y = rep(n_rows + 0.5, 4),
        label = c("", "Pred", "Exp", "Error"),
        stringsAsFactors = FALSE
      ),
      mapping = ggplot2::aes(
        x = x, y = y,
        label = label
      ),
      size = 4.5, fontface = "bold"
    ) +
    
    # Compound names
    ggplot2::geom_text(
      data = data.frame(
        x = rep(0.5, n_rows),
        y = n_rows:1 - 0.5,  # Reverse order for y to match the table image
        label = rownames(df),
        stringsAsFactors = FALSE
      ),
      mapping = ggplot2::aes(
        x = x, y = y,
        label = label
      ),
      size = 4, fontface = "bold"
    ) +
    
    # Predicted values
    ggplot2::geom_text(
      data = data.frame(
        x = rep(1.5, n_rows),
        y = n_rows:1 - 0.5,  # Reverse order for y
        label = df$`OOS Pred`,
        stringsAsFactors = FALSE
      ),
      mapping = ggplot2::aes(
        x = x, y = y,
        label = label
      ),
      size = 4
    ) +
    
    # Measured values
    ggplot2::geom_text(
      data = data.frame(
        x = rep(2.5, n_rows),
        y = n_rows:1 - 0.5,  # Reverse order for y
        label = df$`OOS Measured`,
        stringsAsFactors = FALSE
      ),
      mapping = ggplot2::aes(
        x = x, y = y,
        label = label
      ),
      size = 4
    ) +
    
    # Error values with color coding
    ggplot2::geom_text(
      data = data.frame(
        x = rep(3.5, n_rows),
        y = n_rows:1 - 0.5,  # Reverse order for y
        label = df$`OOS Error`,
        acceptable = df$Acceptable,
        stringsAsFactors = FALSE
      ),
      mapping = ggplot2::aes(
        x = x, y = y,
        label = label,
        color = ifelse(acceptable, "#008000", "#FF0000")
      ),
      size = 4, fontface = "bold"
    ) +
    
    # Use colors directly
    ggplot2::scale_color_identity() +
    
    # Set limits to include headers above the table
    ggplot2::xlim(-0.5, 4.5) +
    ggplot2::ylim(-0.5, n_rows + 1) +
    
    # Theme customization
    ggplot2::theme_void() +
    ggplot2::theme(
      plot.title = ggplot2::element_text(size = 16, face = "bold", hjust = 0.5),
      plot.subtitle = ggplot2::element_text(size = 14, hjust = 0.5),
      # Center the caption with hjust = 0.5 and add top margin
      plot.caption = ggplot2::element_text(size = 10, hjust = 0.5, margin = ggplot2::margin(t = 15)),
      plot.margin = ggplot2::margin(15, 15, 15, 15)
    ) +
    
    # Add title, subtitle and caption
    ggplot2::labs(
      title = plot.title,
      subtitle = subtitle,
      caption = paste0(
        "Mean Absolute Error: ", mae, 
        " | Predictions within threshold: ", accuracy_pct, "%",
        if(nchar(conformation) > 0) paste0("\n", conformation) else ""
      )
    ) +
    
    # Set aspect ratio - make cells rectangular but maintain fixed width-to-height ratio
    ggplot2::coord_fixed(ratio = 0.33, clip = "off")
  
  return(plot)
}

generate_LR_report <- function(dataset,
                               model.list,
                               out.col = 'output',
                               leave.out,
                               what.model = 1,
                               save.pred = T,
                               plot_title = "Linear Regression Model Analysis") {
  default::default(data.frame) <- list(check.names = FALSE)
  mod_data <- data.frame(data.table::fread(dataset, header = T))
  RN <- mod_data[,1]
  mod_data <- mod_data[,-1]
  mod_data <- mod_data[complete.cases(mod_data), ]
  CN <- names(mod_data)
  out.col <- which(CN == out.col)
  mod_data <- data.frame(cbind(scale(mod_data[,-out.col], T, T), mod_data[, out.col]))
  names(mod_data)[1:(ncol(mod_data) - 1)] <- CN[-out.col]
  names(mod_data)[ncol(mod_data)] <- CN[out.col]
  row.names(mod_data) <- RN
  pred_data <- mod_data[row.names(mod_data) %in% leave.out, ]
  mod_data <- mod_data[!(row.names(mod_data) %in% leave.out), ]
  models <- data.frame(data.table::fread(model.list))
  mod.sum <- summary(lm(models$formula[what.model], mod_data))$coefficients
  cat('
  Model Coefficients')
  colnames(mod.sum)[4] <- 'p value'
  k.mod <- knitr::kable(mod.sum)
  print(k.mod)
  cv_3fold <- model.cv(models$formula[what.model], mod_data, dim(mod_data)[2], 3, 50)
  dt3 <- data.frame(cv_3fold[[2]], cv_3fold[[1]])
  names(dt3) <- c('Q2', 'MAE')
  cat('
  3-fold CV')
  tab_dt3 <- knitr::kable(dt3)
  print(tab_dt3)
  cv_5fold <- model.cv(models$formula[what.model], mod_data, dim(mod_data)[2], 5, 50)
  dt5 <- data.frame(cv_5fold[[2]], cv_5fold[[1]])
  names(dt5) <- c('Q2', 'MAE')
  cat('
  5-fold CV')
  tab_dt5 <- knitr::kable(dt5)
  print(tab_dt5)
  
  cv_loo <- model.cv(models$formula[what.model], mod_data, dim(mod_data)[2], nrow(mod_data), 1)
  dtloo <- data.frame(cv_loo[[2]], cv_loo[[1]])
  names(dt3) <- c('Q2', 'MAE')
  cat('
  LOO-CV')
  tab_dtloo <- knitr::kable(dtloo)
  print(tab_dtloo)

  
  mod_data_unn <- data.frame(data.table::fread(dataset, header = T))
  mod.sum.unnormalized <- summary(lm(models$formula[what.model], mod_data_unn))$coefficients
  cat('
  Unnormalized Data Model Coefficients')
  colnames(mod.sum.unnormalized)[4] <- 'p value'
  k.mod.unn <- knitr::kable(mod.sum.unnormalized)
  print(k.mod.unn)
  
  ## model.plot
  info.table <- data.frame(matrix(ncol = 1, nrow = 4))
  info.table[1,1] <- as.character(round(models$R.sq[what.model], 2))
  info.table[2,1] <- as.character(round(dtloo[1, 1], 2))
  info.table[3,1] <- as.character(round(dt5[1, 1], 2))
  info.table[4,1] <- as.character(round(dt3[1, 1], 2))
  row.names(info.table) <-  c('R2', 'Q2_loo', 'Q2_5fold', 'Q2_3fold')
  names(info.table) <- 'stats'
  text1 <- paste(row.names(info.table)[1], info.table[1,1], sep = ' = ')
  text2 <- paste(row.names(info.table)[2], info.table[2,1], sep = ' = ')
  text3 <- paste(row.names(info.table)[3], info.table[3,1], sep = ' = ')
  text4 <- paste(row.names(info.table)[4], info.table[4,1], sep = ' = ')
  
  annotations <- stringr::str_c(c(text1,
                                  text2,
                                  text3,
                                  text4),
                                collapse = "\n")
  
  model = models$formula[what.model]
  best.mod <- lm(model, data = mod_data)
  pred_interval <- predict(best.mod,
                           newdata = mod_data,
                           interval = 'pre',
                           level = 0.9)
  plot.dat <- data.frame(cbind(mod_data[dim(mod_data)[2]], pred_interval))
  colnames(plot.dat) <- c('Measured', 'Predicted', 'lwr', 'upr')
  rownames(plot.dat) <- row.names(mod_data)
  
  row.names(plot.dat) <- stringr::str_replace(row.names(plot.dat),"o_",'2-')
  row.names(plot.dat) <- stringr::str_replace(row.names(plot.dat),"m_",'3-')
  row.names(plot.dat) <- stringr::str_replace(row.names(plot.dat),"p_",'4-')
  row.names(plot.dat) <- stringr::str_replace(row.names(plot.dat),"o4-",'2,4-')
  row.names(plot.dat) <- stringr::str_replace(row.names(plot.dat),"m3-",'3,5-')
  row.names(plot.dat) <- stringr::str_replace(row.names(plot.dat),"o3-",'2,3-')
  
  plot.dat <- dplyr::mutate(plot.dat, Position = rep(NA, nrow(plot.dat)))
  
  for (i in 1:nrow(mod_data)) {
    if (grepl('3-',row.names(plot.dat)[i])) {
      plot.dat[i,5] <- 'meta'
    }
    if (grepl('5-',row.names(plot.dat)[i])) {
      plot.dat[i,5] <- 'meta'
    }
    if (grepl('2-',row.names(plot.dat)[i])) {
      plot.dat[i,5] <- 'ortho'
    }
    if (grepl('basic',row.names(plot.dat)[i])) {
      plot.dat[i,5] <- 'Ph'
    }
    if (grepl('penta_F',row.names(plot.dat)[i])) {
      plot.dat[i,5] <- 'C6F5'
    }
    if (grepl('4-',row.names(plot.dat)[i])) {
      plot.dat[i,5] <- 'para'
    }
  }
  plot.dat <- dplyr::mutate(plot.dat, label = row.names(plot.dat))
  
  plot.dat <- dplyr::mutate(plot.dat, 
                            shapes = c(rep(18, nrow(mod_data))))
  
  
  # Calculate absolute errors
  plot.dat$error <- abs(plot.dat$Measured - plot.dat$Predicted)
  # Create label vector that only includes points with large errors
  plot.dat$show_label <- ifelse(plot.dat$error > 15, plot.dat$label, "")
  
  # Extract dataset name (without path and extension) for subtitle
  dataset_name <- basename(dataset)
  dataset_name <- sub("\\.[^.]*$", "", dataset_name)
  
  # Get model formula as string for subtitle
  model_formula_str <- as.character(models$formula[what.model])
  
  # Store for plot subtitles
  plot_subtitle1 <- paste("Model:", model_formula_str)
  plot_subtitle2 <- paste("Dataset:", dataset_name)
  
  # Calculate plot limits for proper legend positioning and for x/y limits
  x_min <- min(plot.dat[1:nrow(mod_data),3])
  x_max <- max(plot.dat[1:nrow(mod_data),4])
  y_min <- min(plot.dat[1:nrow(mod_data),3])
  y_max <- max(plot.dat[1:nrow(mod_data),4])
  
  plot <- suppressMessages(ggplot2::ggplot(plot.dat, ggplot2::aes(x = Measured, y = Predicted)) +
                             ggplot2::geom_point(size = 2, shape = plot.dat$shapes, ggplot2::aes(color = Position)) +
                             ggplot2::stat_smooth(ggplot2::aes(y = lwr), color = "cadetblue", linetype = "dashed",
                                                  se = F, method = 'lm', fullrange = T, size = 0.8) +
                             ggplot2::stat_smooth(ggplot2::aes(y = upr), color = "cadetblue", linetype = "dashed",
                                                  se = F, method = 'lm', fullrange = T, size = 0.8) +
                             ggplot2::labs(x = 'Measured',
                                           y = 'Predicted',
                                           title = plot_title, 
                                           subtitle = paste(plot_subtitle1, "\n", plot_subtitle2)) +
                             ggplot2::stat_smooth(method = 'lm',se = F, formula = y~x,
                                                  color = 'black',fullrange = T, linetype = 'dashed') +
                             ggplot2::theme(axis.line.x = ggplot2::element_line(linewidth = 1, colour = "black"),
                                            axis.line.y = ggplot2::element_line(linewidth = 1, colour = "black"),
                                            axis.text.x = ggplot2::element_text(colour = "black", size = 12,face = 'bold'),
                                            axis.text.y = ggplot2::element_text(colour = "black", size = 12,face = 'bold'),
                                            axis.title.x = ggplot2::element_text(colour = "black", size = 12,face = 'bold'),
                                            axis.title.y = ggplot2::element_text(colour = "black", size = 12,face = 'bold'),
                                            panel.grid.major = ggplot2::element_blank(),
                                            panel.grid.minor = ggplot2::element_blank(),
                                            panel.border = ggplot2::element_blank(), 
                                            panel.background = ggplot2::element_blank(),
                                            # Position legend at the bottom right inside the plot
                                            legend.position = c(0.85, 0.15),
                                            legend.justification = c(0.5, 0.5),
                                            legend.background = ggplot2::element_blank(),
                                            legend.key = ggplot2::element_blank(),
                                            legend.key.size = unit(0.8, "lines"),
                                            legend.title = ggplot2::element_text(size = 10),
                                            legend.text = ggplot2::element_text(size = 9),
                                            # Title and subtitle styling
                                            plot.title = ggplot2::element_text(size = 14, face = "bold", hjust = 0.5),
                                            plot.subtitle = ggplot2::element_text(size = 10, hjust = 0.5)) +
                             ggplot2::scale_color_manual('', values = c(Ph = "black", meta = 'tan1', C6F5 = 'darkgrey',
                                                                        para = '#66a182',ortho = '#d1495b', external = 'steelblue4')) +
                             ggplot2::xlim(x_min, x_max) +
                             ggplot2::ylim(y_min, y_max) +
                             ggplot2::coord_fixed(ratio = 1) +
                             ggrepel::geom_text_repel(
                               data = subset(plot.dat, error > 13),
                               aes(label = label),
                               size = 3,
                               min.segment.length = Inf,
                               seed = 42,
                               point.padding = 0.4,
                               segment.color = 'grey50',
                               force_pull = 0.02,
                               nudge_x = 0.022,
                               direction = 'y'
                             ) +
                             ggplot2::theme(text = ggplot2::element_text(family = 'Arial')) +
                             ggplot2::annotate('text',
                                               x = min(plot.dat[1:nrow(mod_data),3]),
                                               y = max(plot.dat[1:nrow(mod_data),2]), label = annotations,
                                               parse = F,
                                               hjust = "left", vjust = 0))
  plot
  
  
  prediction <- round(predict(lm(models$formula[what.model], mod_data), pred_data), 0)
  real <- pred_data[, dim(mod_data)[2]]
  RMSE <- round(sqrt((real-prediction)^2), 0)
  prd.tab <- data.frame(prediction, real, RMSE)
  names(prd.tab) <- c('OOS Pred', 'OOS Measured', 'OOS Error')
  if (save.pred == T) write.csv(prd.tab, "OOS_predictions.csv", row.names = TRUE)
  k.prd.tab <- knitr::kable(prd.tab)
  print(k.prd.tab)
  
  print(knitr::kable(data.frame('Mean of RMSE' = mean(RMSE))))
  
  oos_data <- prd.tab[, 1:2]
  
  row.names(oos_data) <- stringr::str_replace(row.names(oos_data),"o_",'2-')
  row.names(oos_data) <- stringr::str_replace(row.names(oos_data),"m_",'3-')
  row.names(oos_data) <- stringr::str_replace(row.names(oos_data),"p_",'4-')
  row.names(oos_data) <- stringr::str_replace(row.names(oos_data),"o4-",'2,4-')
  row.names(oos_data) <- stringr::str_replace(row.names(oos_data),"m3-",'3,5-')
  row.names(oos_data) <- stringr::str_replace(row.names(oos_data),"o3-",'2,3-')
  row.names(oos_data) <- stringr::str_replace(row.names(oos_data),"basic",'Ph')
  row.names(oos_data) <- stringr::str_replace(row.names(oos_data),"penta_F",'C6F5')
  
  print(oos_validation_table(prd.tab))
  
  # Final plot with out-of-sample points
  plot + 
    ggplot2::geom_point(data = oos_data,
                        ggplot2::aes(x = `OOS Measured`, y = `OOS Pred`,
                                     color = "Out-of-Sample"),  # Added color aesthetic
                        size = 1.7,
                        shape = 19) +
    ggplot2::scale_color_manual('', values = c(Ph = "black", 
                                               C6F5 = "darkgrey",
                                               meta = 'tan1',
                                               para = '#66a182', 
                                               ortho = '#d1495b', 
                                               external = 'steelblue4',
                                               "Out-of-Sample" = "cadetblue3")) +  # Added out-of-sample color
    ggrepel::geom_text_repel(data = oos_data,
                             ggplot2::aes(x = `OOS Measured`, y = `OOS Pred`, 
                                          label = rownames(oos_data)),
                             size = 3,
                             min.segment.length = Inf,
                             seed = 42,
                             point.padding = 0.4,
                             segment.color = 'cadetblue3',
                             force_pull = 0.01,
                             nudge_y = -0.022,
                             direction = 'y')
}
```

## Finding best models for all conformers, and selecting the best one

The search procedure starts with searching the best fitted 3-features model for all datasets and using the 
datasets that gave the top results in a 4-feature search. 

```{r}
#inhibitors_names <- read.csv('conformer_0_LasR_above_15.csv')[, 1]

#leave.out <- sample(inhibitors_names, 5) # Done once - Should not run again

# leave.out - should be used from now on - in all LasR analyses
#[1] "o_Naph"  "p_SCF3"  "p_I"     "p_Cl"    "p_azide"
leave.out <- c("o_Naph", "p_SCF3", "p_I", "p_Cl", "p_azide")

all_models_3_3 <- lapply(list.files(pattern = 'LasR.csv'), function(x) models.list.parallel(x, 3, 3, leave.out = leave.out, verbose = F))

names(all_models_3_3) <- list.files(pattern = 'LasR.csv')

all_models_3_3_with_dataset <- lapply(names(all_models_3_3), function(name) {
  df <- all_models_3_3[[name]]
  df$dataset <- name
  return(df)
})


# Combine all dataframes using rbind
combined_all_models_3_3 <- do.call(rbind, all_models_3_3_with_dataset)

# Apply the ranking function to get the top 10 models
top_models_3_3 <- dplyr::arrange(combined_all_models_3_3, desc(Q.sq))[1:10, ]

datasets_for_4_4 <- unique(top_models_3_3$dataset)

all_models_4_4 <- lapply(datasets_for_4_4, function(x) models.list.parallel(x, 4, 4, leave.out = leave.out, verbose = F))

names(all_models_4_4) <- datasets_for_4_4

all_models_4_4_with_dataset <- lapply(names(all_models_4_4), function(name) {
  df <- all_models_4_4[[name]]
  df$dataset <- name
  return(df)
})


# Combine all dataframes using rbind
combined_all_models_4_4 <- do.call(rbind, all_models_4_4_with_dataset)

# Apply the ranking function to get the top 10 models
top_models_4_4 <- dplyr::arrange(combined_all_models_4_4, desc(Q.sq))[1:10, ]

# View the results with key columns
knitr::kable(top_models_4_4)

write.csv(top_models_4_4, 'LasR_top_models_4_4_LO_5.csv')
```


```{r}
model.list <- "LasR_top_models_4_4_LO_5.csv"
out.col = 'output'
what.model = 1
dataset <- read.csv(model.list)$dataset[what.model]

output <- capture.output(generate_LR_report(dataset, model.list, out.col, leave.out, what.model))
for (i in 1:length(output)) print(output[i])
writeLines(output, "output_model_evaluation_LasR.txt")
ggsave('LasR_reg_4_4_LO_5.png', height = 11, width = 11)
```
